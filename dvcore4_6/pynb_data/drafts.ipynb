{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "str = \"Hello, my name is Weiyu. I'm looking for a graduate school\\n Thx!\"\n",
    "# str = str.lower()\n",
    "print(str)\n",
    "\n",
    "str_new = re.split('\\s| , | .',str)\n",
    "print(str_new)\n",
    "str_new = [str for str in str_new if len(str)!=0]\n",
    "print(str_new)\n",
    "str_new = [re.sub(\"[^a-zA-Z0-9']\", '',str) for str in str_new]\n",
    "print(str_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd219bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys = grouped_director_prof.groups.keys()\n",
    "\n",
    "# from openpyxl import Workbook\n",
    "\n",
    "# wb = Workbook()\n",
    "# dest_filename = '/Users/weiyuchen/Downloads/datavis-project-2022-bcd-main/directors.xlsx'\n",
    "\n",
    "# for key in keys:\n",
    "#     sheet_name = key \n",
    "#     # print(type(sheet_name))\n",
    "#     if(len(key)>31): sheet_name = sheet_name[:31]\n",
    "#     wb.create_sheet(sheet_name)\n",
    "\n",
    "# wb.save(filename = dest_filename)\n",
    "\n",
    "# with pd.ExcelWriter('/Users/weiyuchen/Downloads/datavis-project-2022-bcd-main/directors.xlsx', mode = 'w') as writer:  \n",
    "#     for key in keys:\n",
    "#         sheet_name = key \n",
    "#         # print(type(sheet_name))\n",
    "#         if(len(key)>31): sheet_name = sheet_name[:31]\n",
    "#         temp = pd.DataFrame(grouped_director_prof.get_group(key))\n",
    "#         temp.to_excel(writer, sheet_name=sheet_name,float_format=\"%f\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d1209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import http.client\n",
    "# import mimetypes\n",
    "# import re\n",
    "# def getImgLink(id = 'tt0222012'):\n",
    "#     conn = http.client.HTTPSConnection(\"imdb-api.com\", 443)\n",
    "#     payload = ''\n",
    "#     headers = {}\n",
    "#     conn.request(\"GET\", \"/en/API/Title/k_pz2zb0vf/\"+id, payload, headers)\n",
    "#     res = conn.getresponse()\n",
    "#     data = res.read()\n",
    "#     data = data.decode(\"utf-8\")\n",
    "#     # print(type(data))\n",
    "#     print(data)\n",
    "#     first_result = re.findall('\"image\":\"https?://[^\\s]*jpg', data)[0]\n",
    "#     print(first_result[9:])\n",
    "#     return first_result[9:]\n",
    "# getImgLink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03105d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page - Images: https://upload.wikimedia.org/wikipedia/commons/d/d0/2011_Greater_Los_Angeles_Auto_Show_IMG_4306_%286870793840%29.jpg\n",
      "<WikipediaPage 'Hot Wheels'>\n"
     ]
    }
   ],
   "source": [
    "page_py=wikipedia.page('Wheels(film)')\n",
    "print(\"Page - Images: %s\" % page_py.images[0])\n",
    "# print(\"Page - Title: %s\" % wikipedia.summary(\"Wheels(film)\"))\n",
    "# print(\"Page - Summary: %s\" % wikipedia.summary('Wheels(film)', sentences =1))\n",
    "print(page_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_world_link = output_world.copy()\n",
    "# for ind in output_world_link.index:\n",
    "#     output_world_link.at[ind, 0] = getImgLink(output_world_link.at[ind, 0])\n",
    "#     output_world_link.at[ind, 1] = getImgLink(output_world_link.at[ind, 1])\n",
    "# print(output_world_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03a591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Weiyu. I'm looking for a graduate school\n",
      " Thx!\n",
      "['Hello,', 'my', 'name', 'is', 'Weiyu.', \"I'm\", 'looking', 'for', 'a', 'graduate', 'school', '', 'Thx!']\n",
      "['Hello,', 'my', 'name', 'is', 'Weiyu.', \"I'm\", 'looking', 'for', 'a', 'graduate', 'school', 'Thx!']\n",
      "['Hello', 'my', 'name', 'is', 'Weiyu', \"I'm\", 'looking', 'for', 'a', 'graduate', 'school', 'Thx']\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "\n",
    "# str = \"Hello, my name is Weiyu. I'm looking for a graduate school\\n Thx!\"\n",
    "# # str = str.lower()\n",
    "# print(str)\n",
    "\n",
    "# str_new = re.split('\\s| , | .',str)\n",
    "# print(str_new)\n",
    "# str_new = [str for str in str_new if len(str)!=0]\n",
    "# print(str_new)\n",
    "# str_new = [re.sub(\"[^a-zA-Z0-9']\", '',str) for str in str_new]\n",
    "# print(str_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8878ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "\n",
    "class Crawler_google_images:\n",
    "    # 初始化\n",
    "    def __init__(self, keyword):\n",
    "        self.url = 'https://www.bing.com/images/search?q='+keyword +' film poster'+'&tbm=isch'\n",
    "        self.keyword = keyword\n",
    "\n",
    "    # 获得Chrome驱动，并访问url\n",
    "    def init_browser(self):\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--disable-infobars\")\n",
    "        browser = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        # 访问url\n",
    "        browser.get(self.url)\n",
    "        # 最大化窗口，之后需要爬取窗口中所见的所有图片\n",
    "        browser.maximize_window()\n",
    "        return browser\n",
    "\n",
    "    #下载图片\n",
    "    def download_images(self, browser,round=2):\n",
    "        picpath = './images/'\n",
    "        # 路径不存在时创建一个\n",
    "        if not os.path.exists(picpath): os.makedirs(picpath)\n",
    "\n",
    "        count = 0 #图片序号\n",
    "        pos = 0\n",
    "        for i in range(round):\n",
    "            pos += 500\n",
    "            # 向下滑动\n",
    "            js = 'var q=document.documentElement.scrollTop=%d'%pos\n",
    "            browser.execute_script(js)\n",
    "            time.sleep(1)\n",
    "            # 找到图片\n",
    "            # html = browser.page_source#也可以抓取当前页面的html文本，然后用beautifulsoup来抓取\n",
    "            #直接通过tag_name来抓取是最简单的，比较方便\n",
    "\n",
    "            img_elements = browser.find_elements(By.CLASS_NAME, 'iusc')\n",
    "            # print(img_elements)\n",
    "            for img_element in img_elements:\n",
    "                m = img_element.get_attribute('m')\n",
    "                # print(m)\n",
    "                img_url = re.findall(\"murl\\\":\\\"https?://[^\\s]*.jpg\", m)\n",
    "                # print(img_url[0][7:])\n",
    "                img_url = img_url[0][7:]\n",
    "                filename = picpath+'/'+ self.keyword + \".jpg\"\n",
    "                r = requests.get(img_url)\n",
    "                img = Image.open(BytesIO(r.content))\n",
    "                width, height = img.size\n",
    "                print(\"width: \", width , \", height: \", height)\n",
    "                if(height>width):\n",
    "                    with open(filename, 'wb') as f:\n",
    "                        f.write(r.content)\n",
    "                    f.close()\n",
    "                    count += 1\n",
    "                    print('this is '+'img %d'%count)\n",
    "                #防止反爬机制\n",
    "                time.sleep(0.2)\n",
    "                if(count>0): break\n",
    "            if(count>0): break\n",
    "    def run(self):\n",
    "        browser = self.init_browser()\n",
    "        self.download_images(browser,10)#可以修改爬取的页面数，基本10页是100多张图片\n",
    "        browser.close()\n",
    "        print(\"爬取完成\")\n",
    "\n",
    "\n",
    "def fetchImage(keyword = 'Your Name.'):\n",
    "    craw = Crawler_google_images(keyword)\n",
    "    craw.run()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
