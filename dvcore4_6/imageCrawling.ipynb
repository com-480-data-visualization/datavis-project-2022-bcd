{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from urllib import error\n",
    "from urllib import request\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# default url\n",
    "# replace for yours\n",
    "url = \"https://www.bing.com/?scope=images&nr=1&FORM=NOFORM\"\n",
    "explorer = \"Safari\"\n",
    "# directory\n",
    "imgs_dir = \"./images/\"\n",
    "\n",
    "\n",
    "# report hook with three parameters passed\n",
    "# count_of_blocks  The number of blocks transferred\n",
    "# block_size The size of block\n",
    "# total_size Total size of the file\n",
    "def progress_callback(count_of_blocks, block_size, total_size):\n",
    "    # determine current progress\n",
    "    progress = int(50 * (count_of_blocks * block_size) / total_size)\n",
    "    if progress > 50:\n",
    "        progress = 50\n",
    "    # update progress bar\n",
    "    sys.stdout.write(\"\\r[%s%s] %d%%\" % ('█' * progress, '  ' * (50 - progress), progress * 2))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "class CrawlSelenium:\n",
    "\n",
    "\tdef __init__(self, keyword, explorer=\"Safari\", url=\"https://www.google.com\"):\n",
    "\t\tself.url = url\n",
    "\t\tself.explorer = explorer\n",
    "\t\tself.keyword = keyword\n",
    "\n",
    "\tdef set_loading_strategy(self, strategy=\"normal\"):\n",
    "\t\tself.options = Options()\n",
    "\t\tself.options.page_load_strategy = strategy\n",
    "\n",
    "\n",
    "\tdef crawl(self):\n",
    "\t\t# instantiate driver according to corresponding explorer\n",
    "\t\tif self.explorer == \"Chrome\":\n",
    "\t\t\tself.driver = webdriver.Chrome(options=self.options)\n",
    "\t\tif self.explorer == \"Opera\":\n",
    "\t\t\tself.driver = webdriver.Opera(options=self.options)\n",
    "\t\tif self.explorer == \"Firefox\":\n",
    "\t\t\tself.driver = webdriver.Firefox(options=self.options)\n",
    "\t\tif self.explorer == \"Edge\":\n",
    "\t\t\tself.driver = webdriver.Edge(options=self.options)\n",
    "\t\tif self.explorer == \"Safari\":\n",
    "\t\t\tself.driver = webdriver.Safari(options=self.options)\n",
    "\n",
    "\t\t# search on google\n",
    "\t\t# navigate to url\n",
    "\t\tself.driver.get(self.url)\n",
    "\t\t# locate input field\n",
    "\t\tsearch_input = self.driver.find_element(By.NAME, 'q')\n",
    "\t\t# emulate user input and enter to search\n",
    "\t\twebdriver.ActionChains(self.driver).move_to_element(search_input).send_keys(self.keyword + Keys.ENTER).perform()\n",
    "\t\t\n",
    "\t\t# self.driver.find_element(By.LINK_TEXT, '我同意').click()\n",
    "\t\t# # navigate to google image\n",
    "\t\t# # find navigation buttons\n",
    "\t\t# self.driver.find_element(By.LINK_TEXT, '图片').click()\n",
    "\n",
    "\t\t# load more images as many as possible\n",
    "\t\t# scrolling to bottom\n",
    "\t\tself.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "\t\t# get button\n",
    "\t\t# show_more_button = self.driver.find_element(By.CSS_SELECTOR, \"input[value='显示更多搜索结果']\")\n",
    "\t\t# try:\n",
    "\t\t# \twhile True:\n",
    "\t\t# \t\t# do according to message\n",
    "\t\t# \t\tmessage = self.driver.find_element(By.CSS_SELECTOR, 'div.OuJzKb.Bqq24e').get_attribute('textContent')\n",
    "\t\t# \t\t# print(message)\n",
    "\t\t# \t\tif message == '正在加载更多内容，请稍候':\n",
    "\t\t# \t\t\tself.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "\t\t# \t\telif message == '新内容已成功加载。向下滚动即可查看更多内容。':\n",
    "\t\t# \t\t\t# scrolling to bottom\n",
    "\t\t# \t\t\tself.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "\t\t# \t\t\tif show_more_button.is_displayed():\n",
    "\t\t# \t\t\t\tshow_more_button.click()\n",
    "\t\t# \t\telif message == '看来您已经看完了所有内容':\n",
    "\t\t# \t\t\tbreak\n",
    "\t\t# \t\telif message == '无法加载更多内容，点击即可重试。':\n",
    "\t\t# \t\t\tshow_more_button.click()\n",
    "\t\t# \t\telse:\n",
    "\t\t# \t\t\tself.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "\t\t# except Exception as err:\n",
    "\t\t# \tprint(err)\n",
    "\t\tmessage = self.driver.find_element(By.CSS_SELECTOR, 'div.OuJzKb.Bqq24e').get_attribute('textContent')\n",
    "\t\tprint(message)\n",
    "\t\t# find all image elements in google image result page\n",
    "\t\timgs = self.driver.find_elements(By.CSS_SELECTOR, \"img.rg_i.Q4LuWd\")\n",
    "\t\t\n",
    "\t\timg_count = 0\n",
    "\t\tfor img in imgs:\n",
    "\t\t\ttry:\n",
    "\t\t\t\t# image per second\n",
    "\t\t\t\ttime.sleep(1)\n",
    "\t\t\t\t\n",
    "\t\t\t\timg_url = img.get_attribute(\"src\")\n",
    "\t\t\t\tif img_url == None:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tpath = os.path.join(imgs_dir,keyword,'/',str(img_count) + \"_img.jpg\")\n",
    "\t\t\t\trequest.urlretrieve(url = img_url, filename = path, reporthook = progress_callback, data = None)\n",
    "\t\t\t\tprint('\\ndownloaded image ' + str(img_count) + ': ')\n",
    "\t\t\t\timg_count = img_count + 1\n",
    "\t\t\t\tif(img_count>2): break\n",
    "\t\t\texcept error.HTTPError as http_err:\n",
    "\t\t\t\tprint(http_err)\n",
    "\t\t\texcept Exception as err:\n",
    "\t\t\t\tprint(err)\n",
    "\n",
    "\n",
    "def main():\n",
    "\t# setting\n",
    "\tkeyword = 'Hera Pheri' + ' film imdb'\n",
    "\tcrawl_s = CrawlSelenium(keyword, explorer, url)\n",
    "\tcrawl_s.set_loading_strategy(\"normal\")\n",
    "\t# make directory\n",
    "\tif not os.path.exists(imgs_dir):\n",
    "\t\tos.mkdir(imgs_dir)\n",
    "\t# crawling\n",
    "\tcrawl_s.crawl()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Crawler_google_images:\n",
    "    # 初始化\n",
    "    def __init__(self, keyword, url):\n",
    "        self.url = url\n",
    "        self.keyword = keyword\n",
    "\n",
    "    # 获得Chrome驱动，并访问url\n",
    "    def init_browser(self):\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--disable-infobars\")\n",
    "        browser = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        # 访问url\n",
    "        browser.get(self.url)\n",
    "        # 最大化窗口，之后需要爬取窗口中所见的所有图片\n",
    "        browser.maximize_window()\n",
    "        return browser\n",
    "\n",
    "    #下载图片\n",
    "    def download_images(self, browser,round=2):\n",
    "        picpath = './images/'+keyword\n",
    "        # 路径不存在时创建一个\n",
    "        if not os.path.exists(picpath): os.makedirs(picpath)\n",
    "        # 记录下载过的图片地址，避免重复下载\n",
    "        img_url_dic = []\n",
    "\n",
    "        count = 0 #图片序号\n",
    "        pos = 0\n",
    "        for i in range(round):\n",
    "            pos += 500\n",
    "            # 向下滑动\n",
    "            js = 'var q=document.documentElement.scrollTop=' + str(pos)\n",
    "            browser.execute_script(js)\n",
    "            time.sleep(1)\n",
    "            # 找到图片\n",
    "            # html = browser.page_source#也可以抓取当前页面的html文本，然后用beautifulsoup来抓取\n",
    "            #直接通过tag_name来抓取是最简单的，比较方便\n",
    "\n",
    "            img_elements = browser.find_elements_by_tag_name('img')\n",
    "            #遍历抓到的webElement\n",
    "            for img_element in img_elements:\n",
    "                img_url = img_element.get_attribute('src')\n",
    "                # print(img_url)\n",
    "                # 前几个图片的url太长，不是图片的url，先过滤掉，爬后面的\n",
    "                if isinstance(img_url, str):\n",
    "                    if len(img_url) <= 200:\n",
    "                        #将干扰的goole图标筛去\n",
    "                        if 'images' in img_url:\n",
    "                            #判断是否已经爬过，因为每次爬取当前窗口，或许会重复\n",
    "                            #实际上这里可以修改一下，将列表只存储上一次的url，这样可以节省开销，不过我懒得写了···\n",
    "                            if img_url not in img_url_dic:\n",
    "                                try:\n",
    "                                    img_url_dic.append(img_url)\n",
    "                                    #下载并保存图片到当前目录下\n",
    "                                    filename = picpath+'/'+ str(count) + \".jpg\"\n",
    "                                    r = requests.get(img_url)\n",
    "                                    # print(\"r: \", r)\n",
    "                                    # print(\"Trying to open the image\")\n",
    "                                    img = Image.open(BytesIO(r.content))\n",
    "                                    width, height = img.size\n",
    "                                    print(\"width: \", width , \", height: \", height)\n",
    "                                    if(height>width):\n",
    "                                        with open(filename, 'wb') as f:\n",
    "                                            f.write(r.content)\n",
    "                                        f.close()\n",
    "                                        count += 1\n",
    "                                        print('this is '+'img '+str(count))\n",
    "                                    \n",
    "                                    #防止反爬机制\n",
    "                                    time.sleep(0.2)\n",
    "                                except:\n",
    "                                    print('failure')\n",
    "                if(count>2): break\n",
    "\n",
    "    def run(self):\n",
    "        browser = self.init_browser()\n",
    "        self.download_images(browser,10)#可以修改爬取的页面数，基本10页是100多张图片\n",
    "        browser.close()\n",
    "        print(\"爬取完成\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    keyword = 'Your Name.' + ' imdb poster'\n",
    "    url = 'https://www.google.com.hk/search?q='+keyword+'&tbm=isch'\n",
    "    print(url)\n",
    "    craw = Crawler_google_images(keyword, url)\n",
    "    craw.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "\n",
    "class Crawler_google_images:\n",
    "    # 初始化\n",
    "    def __init__(self, keyword, url):\n",
    "        self.url = url\n",
    "        self.keyword = keyword\n",
    "\n",
    "    # 获得Chrome驱动，并访问url\n",
    "    def init_browser(self):\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--disable-infobars\")\n",
    "        browser = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        # 访问url\n",
    "        browser.get(self.url)\n",
    "        # 最大化窗口，之后需要爬取窗口中所见的所有图片\n",
    "        browser.maximize_window()\n",
    "        return browser\n",
    "\n",
    "    #下载图片\n",
    "    def download_images(self, browser,round=2):\n",
    "        picpath = './images/'+keyword\n",
    "        # 路径不存在时创建一个\n",
    "        if not os.path.exists(picpath): os.makedirs(picpath)\n",
    "        # 记录下载过的图片地址，避免重复下载\n",
    "        img_url_dic = []\n",
    "\n",
    "        count = 0 #图片序号\n",
    "        pos = 0\n",
    "        for i in range(round):\n",
    "            pos += 500\n",
    "            # 向下滑动\n",
    "            js = 'var q=document.documentElement.scrollTop=' + str(pos)\n",
    "            browser.execute_script(js)\n",
    "            time.sleep(1)\n",
    "            # 找到图片\n",
    "            # html = browser.page_source#也可以抓取当前页面的html文本，然后用beautifulsoup来抓取\n",
    "            #直接通过tag_name来抓取是最简单的，比较方便\n",
    "\n",
    "            img_elements = browser.find_elements(By.CLASS_NAME, 'iusc')\n",
    "            # print(img_elements)\n",
    "            for img_element in img_elements:\n",
    "                m = img_element.get_attribute('m')\n",
    "                print(m)\n",
    "                img_url = re.findall(\"murl\\\":\\\"https?://[^\\s]*.jpg\", m)\n",
    "                print(img_url[0][7:])\n",
    "            #遍历抓到的webElement\n",
    "            # for img_element in img_elements:\n",
    "            #     img_url = img_element.get_attribute('src')\n",
    "            #     # print(img_url)\n",
    "            #     # 前几个图片的url太长，不是图片的url，先过滤掉，爬后面的\n",
    "            #     if isinstance(img_url, str):\n",
    "            #         if len(img_url) <= 200:\n",
    "            #             #将干扰的goole图标筛去\n",
    "            #             if 'images' in img_url:\n",
    "            #                 #判断是否已经爬过，因为每次爬取当前窗口，或许会重复\n",
    "            #                 #实际上这里可以修改一下，将列表只存储上一次的url，这样可以节省开销，不过我懒得写了···\n",
    "            #                 if img_url not in img_url_dic:\n",
    "            #                     try:\n",
    "            #                         img_url_dic.append(img_url)\n",
    "            #                         #下载并保存图片到当前目录下\n",
    "            #                         filename = picpath+'/'+ str(count) + \".jpg\"\n",
    "            #                         r = requests.get(img_url)\n",
    "            #                         # print(\"r: \", r)\n",
    "            #                         # print(\"Trying to open the image\")\n",
    "            #                         img = Image.open(BytesIO(r.content))\n",
    "            #                         width, height = img.size\n",
    "            #                         print(\"width: \", width , \", height: \", height)\n",
    "            #                         if(height>width):\n",
    "            #                             with open(filename, 'wb') as f:\n",
    "            #                                 f.write(r.content)\n",
    "            #                             f.close()\n",
    "            #                             count += 1\n",
    "            #                             print('this is '+'img '+str(count))\n",
    "                                    \n",
    "            #                         #防止反爬机制\n",
    "            #                         time.sleep(0.2)\n",
    "            #                     except:\n",
    "            #                         print('failure')\n",
    "            #     if(count>2): break\n",
    "\n",
    "    def run(self):\n",
    "        browser = self.init_browser()\n",
    "        self.download_images(browser,10)#可以修改爬取的页面数，基本10页是100多张图片\n",
    "        browser.close()\n",
    "        print(\"爬取完成\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    keyword = 'Your Name.' + ' film poster'\n",
    "    url = 'https://www.bing.com/images/search?q='+keyword+'&tbm=isch'\n",
    "    print(url)\n",
    "    craw = Crawler_google_images(keyword, url)\n",
    "    craw.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a class=\"iusc\" \n",
    "style=\"height:279px;width:186px\" \n",
    "m=\"{&quot;sid&quot;:&quot;&quot;,&quot;cturl&quot;:&quot;&quot;,&quot;cid&quot;:&quot;u7VulGqy&quot;,&quot;purl&quot;:&quot;https://www.alternateending.com/2017/04/your-name-2016.html&quot;,&quot;murl&quot;:&quot;https://www.alternateending.com/wp-content/uploads/2017/04/l9KxbI1LaCXtuEOtZe8P2uNOeAR-1024x1536.jpg&quot;,&quot;turl&quot;:&quot;https://tse2.mm.bing.net/th?id=OIP.u7VulGqy2EbnU4Htk3GwYAHaLH&amp;pid=15.1&quot;,&quot;md5&quot;:&quot;bbb56e946ab2d846e75381ed9371b060&quot;,&quot;shkey&quot;:&quot;b2X7CY8MNiHQ565QOTRWwUAIQAT9UBEuM5DVY7IYChQ=&quot;,&quot;t&quot;:&quot;Your Name. (2016) - Movie Review : Alternate Ending&quot;,&quot;mid&quot;:&quot;8127F546E86BE3F755AF4963D96FD40CBA65AD31&quot;,&quot;desc&quot;:&quot;yourname shinkai makoto alternateending sinopse&quot;}\" \n",
    "onclick=\"sj_evt.fire('IFrame.Navigate', this.href); return false;\" \n",
    "href=\"/images/search?view=detailV2&amp;ccid=u7VulGqy&amp;id=8127F546E86BE3F755AF4963D96FD40CBA65AD31&amp;thid=OIP.u7VulGqy2EbnU4Htk3GwYAHaLH&amp;mediaurl=https%3a%2f%2fwww.alternateending.com%2fwp-content%2fuploads%2f2017%2f04%2fl9KxbI1LaCXtuEOtZe8P2uNOeAR-1024x1536.jpg&amp;cdnurl=https%3a%2f%2fth.bing.com%2fth%2fid%2fR.bbb56e946ab2d846e75381ed9371b060%3frik%3dMa1lugzUb9ljSQ%26pid%3dImgRaw%26r%3d0&amp;exph=1536&amp;expw=1024&amp;q=Your+Name.+film+poster&amp;simid=607989334476718169&amp;FORM=IRPRST&amp;ck=4F31AC3C20993CB869B9952E4E55CA67&amp;selectedIndex=0\" \n",
    "h=\"ID=images,5183.1\" \n",
    "data-focevt=\"1\">\n",
    "\n",
    "https://www.alternateending.com/wp-content/uploads/2017/04/l9KxbI1LaCXtuEOtZe8P2uNOeAR-1024x1536.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date：2020.5.25\n",
    "# author:pmy\n",
    "# aim:爬取google图片\n",
    "#问题在于，不能保证所爬为所见\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "\n",
    "class Crawler_google_images:\n",
    "    # 初始化\n",
    "    def __init__(self, keyword, url):\n",
    "        self.url = url\n",
    "        self.keyword = keyword\n",
    "\n",
    "    # 获得Chrome驱动，并访问url\n",
    "    def init_browser(self):\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--disable-infobars\")\n",
    "        browser = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        # 访问url\n",
    "        browser.get(self.url)\n",
    "        # 最大化窗口，之后需要爬取窗口中所见的所有图片\n",
    "        browser.maximize_window()\n",
    "        return browser\n",
    "\n",
    "    # 下载图片\n",
    "    def download_images(self, browser, num=100):\n",
    "        #存储路径\n",
    "        picpath = './images/'+keyword\n",
    "        # 路径不存在时创建一个\n",
    "        if not os.path.exists(picpath): os.makedirs(picpath)\n",
    "\n",
    "        count = 0  # 图片序号\n",
    "        pos = 0\n",
    "        # print(num)\n",
    "\n",
    "        while (True):\n",
    "            # try:\n",
    "            #     # 向下滑动\n",
    "            #     js = 'var q=document.documentElement.scrollTop=' + str(pos)\n",
    "            #     pos += 10\n",
    "            #     browser.execute_script(js)\n",
    "            #     time.sleep(1)\n",
    "            #     # 找到图片\n",
    "            #     # html = browser.page_source#也可以抓取当前页面的html文本，然后用beautifulsoup来抓取\n",
    "            #     # 直接通过tag_name来抓取是最简单的，比较方便\n",
    "            # except:\n",
    "            #     print(\"划不动了\")\n",
    "            img_elements = browser.find_elements(By.XPATH, '//a[@class=\"wXeWr islib nfEiy mM5pbd\"]')\n",
    "            try:\n",
    "                for img_element in img_elements:\n",
    "                    #点开大图页面\n",
    "                    img_element.click()\n",
    "                    time.sleep(0.5)\n",
    "                    try:\n",
    "                        # 这里balabala里面有好几个，所以要过滤一下\n",
    "                        # 取名好烦哦···\n",
    "                        balabalas = browser.find_elements(By.XPATH,'//img[@class=\"n3VNCb\"]')\n",
    "\n",
    "                        if (balabalas):\n",
    "                            for balabala in balabalas:\n",
    "                                src = balabala.get_attribute('src')\n",
    "                                #过滤掉缩略图和无关干扰信息\n",
    "                                if src.startswith('http') and not src.startswith(\n",
    "                                        'https://encrypted-tbn0.gstatic.com'):\n",
    "                                    print('Found' + str(count) + 'st image url')\n",
    "                                    # img_url_dic.append(src)\n",
    "                                    self.save_img(count, src, picpath)\n",
    "                                    count += 1\n",
    "                                    #爬取到指定数量图片后退出\n",
    "                                    if (count >= num):\n",
    "                                        return \"stop\"\n",
    "                    except:\n",
    "                        print('获取图片失败')\n",
    "                #回退\n",
    "                browser.back()\n",
    "                time.sleep(0.3)\n",
    "            except:\n",
    "                print('获取页面失败')\n",
    "\n",
    "    def save_img(self, count, img_src, picpath):\n",
    "        filename = picpath + '/' + str(count) + '.jpg'\n",
    "        r = requests.get(img_src)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        f.close()\n",
    "\n",
    "    def run(self):\n",
    "        browser = self.init_browser()\n",
    "        self.download_images(browser, 10)  # 可以修改爬取的图片数\n",
    "        browser.close()\n",
    "        print(\"############爬取完成\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    keyword = 'Hera Pheri' + ' film imdb poster'\n",
    "    url = 'https://www.google.com.hk/search?q='+keyword+'&tbm=isch'\n",
    "    craw = Crawler_google_images(keyword, url)\n",
    "    craw.run()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.bing.com/images/search?q=Your Name. film poster&tbm=isch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-35a4b4ef0584>:22: DeprecationWarning: use options instead of chrome_options\n",
      "  browser = webdriver.Chrome(chrome_options=chrome_options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width:  1024 , height:  1536\n",
      "this is img 1\n",
      "width:  474 , height:  712\n",
      "this is img 2\n",
      "width:  1600 , height:  2400\n",
      "this is img 3\n",
      "爬取完成\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "\n",
    "class Crawler_google_images:\n",
    "    # 初始化\n",
    "    def __init__(self, keyword, url):\n",
    "        self.url = url\n",
    "        self.keyword = keyword\n",
    "\n",
    "    # 获得Chrome驱动，并访问url\n",
    "    def init_browser(self):\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--disable-infobars\")\n",
    "        browser = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        # 访问url\n",
    "        browser.get(self.url)\n",
    "        # 最大化窗口，之后需要爬取窗口中所见的所有图片\n",
    "        browser.maximize_window()\n",
    "        return browser\n",
    "\n",
    "    #下载图片\n",
    "    def download_images(self, browser,round=2):\n",
    "        picpath = './images/'+keyword\n",
    "        # 路径不存在时创建一个\n",
    "        if not os.path.exists(picpath): os.makedirs(picpath)\n",
    "\n",
    "        count = 0 #图片序号\n",
    "        pos = 0\n",
    "        for i in range(round):\n",
    "            pos += 500\n",
    "            # 向下滑动\n",
    "            js = 'var q=document.documentElement.scrollTop=' + str(pos)\n",
    "            browser.execute_script(js)\n",
    "            time.sleep(1)\n",
    "            # 找到图片\n",
    "            # html = browser.page_source#也可以抓取当前页面的html文本，然后用beautifulsoup来抓取\n",
    "            #直接通过tag_name来抓取是最简单的，比较方便\n",
    "\n",
    "            img_elements = browser.find_elements(By.CLASS_NAME, 'iusc')\n",
    "            # print(img_elements)\n",
    "            for img_element in img_elements:\n",
    "                m = img_element.get_attribute('m')\n",
    "                # print(m)\n",
    "                img_url = re.findall(\"murl\\\":\\\"https?://[^\\s]*.jpg\", m)\n",
    "                # print(img_url[0][7:])\n",
    "                filename = picpath+'/'+ str(count) + \".jpg\"\n",
    "                r = requests.get(img_url[0][7:])\n",
    "                img = Image.open(BytesIO(r.content))\n",
    "                width, height = img.size\n",
    "                print(\"width: \", width , \", height: \", height)\n",
    "                if(height>width):\n",
    "                    with open(filename, 'wb') as f:\n",
    "                        f.write(r.content)\n",
    "                    f.close()\n",
    "                    count += 1\n",
    "                    print('this is '+'img '+str(count))\n",
    "                #防止反爬机制\n",
    "                time.sleep(0.2)\n",
    "                if(count>2): break\n",
    "            if(count>2): break\n",
    "    def run(self):\n",
    "        browser = self.init_browser()\n",
    "        self.download_images(browser,10)#可以修改爬取的页面数，基本10页是100多张图片\n",
    "        browser.close()\n",
    "        print(\"爬取完成\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    keyword = 'Your Name.' + ' film poster'\n",
    "    url = 'https://www.bing.com/images/search?q='+keyword+'&tbm=isch'\n",
    "    print(url)\n",
    "    craw = Crawler_google_images(keyword, url)\n",
    "    craw.run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7dd161eb1024325ae7f7c75f83468422f686aa604b25f273800098b62b71e090"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
